\documentclass{article}
\usepackage{./nips15submit_e,amsmath,amsthm}
%\usepackage{amsmath,amssymb,enumerate,mathtools}

\newcommand{\eqnref}[1]{equation \eqref{eq:#1}}
\newcommand{\Eqnref}[1]{Equation \eqref{eq:#1}}
\newcommand{\secref}[1]{section \ref{sec:#1}}
\newcommand{\Secref}[1]{Section \ref{sec:#1}}

\newtheorem{theorem}{Theorem}

\title{Private Attribution Networks}
\author{
Maxwell Anselm\\
Lehigh University\\
Bethlehem, PA 18015\\
\texttt{mba210@lehigh.edu}\\
}

\nipsfinalcopy
\begin{document}
\maketitle

\section{Introduction}

motivation, summary, takeaway

Peer-to-peer (P2P) networks are a common tool for sharing information. Most P2P
architectures are completely open: the identities of the participants and the
data available in the network are available to the public. When privacy is
applied ot P2P networks, it is usually done in a way that completely closes down
the network: by making membership in the network invite-only\cite{privatep2p}.

We define a new kind of privacy for P2P networks called ``attribution privacy''
which exists in a traditional open network. Attribution privacy formalizes
the notion that sharing public information among public peers can still leak
private information. We give examples of both real world and technical
applications where attribution privacy is a concern. We also classify when
attribution privacy leaks can occur and measure the potential risk to peers
participating in such a network.

We consider several modifications that can be made to the P2P protocol in order
to mitigate the attribution privacy leaks, and measure their efficacy as well as
their performance impact. Lastly we define a private attribution network, which
combines these methods to provide a balance of privacy and performance.

\section{Related Work}

XRay, Chord (P2P), TOR, BitTorrent

\section*{Main sections follow}

solution, implementation, main challenges overcome

\section{Definitions}

We first define what information sharing means in our P2P network. We
imagine a P2P network in which there are a fixed number of peers (the
swarm) and all peers are aware of the other members of the swarm via some
central authority (the tracker). These peers exchange messages via some secure
channel such as HTTPS (perhaps using public keys synchronised through the
tracker). Thus which peers are communicating with which is public information,
while the contents of those messages is kept private from an observer.

Let $\mathcal{Q}$ be a set of public queries and let $\mathcal{D}$ be a set of
public data about those queries. Intuitively, the elements of $\mathcal{Q}$
represent the queries that a peer might make to other peers in the swarm, and
$\mathcal{D}$ represents potential responses to those queries. Then we define a
function $K:\mathcal{Q}\rightarrow 2^\mathcal{D}$ which maps queries to the set
of possible responses to each query i.e. $\forall q\in\mathcal{Q},
K(q)\subseteq\mathcal{D}$. The reason why peers participate in the network is to
learn more about the ``knowledge function'' $K$. For example, we can imagine
$\mathcal{Q}$ as a set of file names and $\mathcal{D}$ as a set of file
fragments; then $K$ indicates which fragments correspond to which file names (and
which are reused across multiple file names), so determining the value of $K(q)$
amounts to downloading file $q$. Formally, a response $D\subseteq\mathcal{D}$ to
a query $q\in\mathcal{Q}$ indicates that $D\subseteq K(q)$.

Next we define $\mathcal{S}$ to be the set of original sources of knowledge
about $K$ in the network. If a peer possesses some source $s\in\mathcal{S}$,
then they can receive some information $D_s\subseteq K(q_s)$ {\it without}
querying another peer in the network. In this case we say that the peer {\it
originates} (or is the {\it origin} of) $D_s\subseteq K(q_s)$. We assume that
the network is otherwise closed, so if a peer wants to learn about $K(q)$ for
some $q$, there need to be peers in the network with sources pertaining to $q$.

For attribution privacy to come into play we need two conditions to hold:
\begin{enumerate}
	\item The sources must be private. That is, a peer with access to some
		$s\in\mathcal{S}$ does not want it to be known publicly.
	\item The relationship between $D_s\subseteq K(q_s)$ and $s\in\mathcal{S}$
		must be public. That is, a peer who sees the response $D\subseteq K(q)$
		will know which sources in $\mathcal{S}$ it might have originated from.
\end{enumerate}

If these conditions hold, then it is possible for a public peer to share public
data about a public query, and in doing so inadvertently communicate information
about its private sources. That is an attribution privacy leak.

\section{Applications}\label{sec:example}

\subsection{Nickelback}

A simple (somewhat facetious) example is of a small town in which no one likes
the Canadian rock band Nickelback. Here we imagine the townspeople as peers in
the P2P network. If you (being a huge Nickelback fan) were to move to this town,
you might find yourself in the uncomfortable situation where a Nickelback song
comes on the radio and people are curious what the name is and who performs it.
In this situation, $\mathcal{S}$ contains the property ``being a Nickelback
fan'' (a source of knowledge about Nickelback songs). $\mathcal{Q}$ contains the
unidentified song on the radio, $\mathcal{D}$ contains song names, and $K$ maps
the unidentified song to the correct name. The curious members of the town query
for name of the song, and you would honestly like to provide it.

However, being that you are the only Nickelback fan in town, doing so would
prove to whomever you tell that you {\it originate} the knowledge of the correct
song name. The townspeople would be able to apply their public knowledge of the
sources to discover your private information: namely that a person who knows the
name of a random Nickelback song is probably a Nickelback fan. By honestly
providing the requested data, you have inadvertently revealed your private
source. This is a P2P network where private attribution is needed.

\subsection{XRay}

For a more technical example, consider the tool XRay\cite{xray}. XRay audits
online advertisements to reverse-engineer the ad targeting criteria and thus
determine if a company is exploiting one's sensitive information. It does this
using a complicated system of data duplication and fake account management, but
the authors note that ``a collaborative approach to auditing, in which users
contribute their [data] in a privacy-preserving way is a promising
direction...'' In such a system, users would participate in a P2P network where
they freely share which ads they have seen and in association with which
personal information. Then the network could collaborate to determine the ad
targeting criteria.

In this situation, $\mathcal{Q}$ is the set of online ads, $\mathcal{D}$ is the
set of personal information that might be targeted, and $K$ represents the ad
targeting criteria i.e. which personal data are targeted by which ads.
Interestingly, the private sources $\mathcal{S}$ in this case are identical to
$\mathcal{D}$: if you originate the knowledge that a particular ad targets
particular data, then informing others of that association also divulges that
you possess the targeted data. Such a P2P network must consider its members'
attribution privacy.

\section{Privacy Risk}
\subsection{Baseline}\label{sec:risk}

To measure the attribution privacy risk in a P2P network, we start by
considering the worst-case scenario. Suppose that for knowledge about some $q$
there is only a single source $s$, only a single peer $x$ with access to that
source (i.e. a single origin), and that $x$ has not shared any knowledge of $q$
with the rest of the network. Another peer $y$ who wants to learn about $K(q)$
might then query {\it every peer} in the network for knowledge about $q$. Since
$x$ has not previously shared information about $q$, every peer will respond
with no knowledge except for $x$.

$y$ now knows that of all the peers, only $x$ has knowledge about $q$. Because
the network is closed, $y$ also knows that there must be a source for $q$ among
the peers who know about $q$. Thus $x$ must possess a source for $q$ and since
we assumed there was only one such source, it must be $s$. Thus by sharing their
knowledge about $q$, $x$ has inadvertently shared the fact that they possess
source $s$.

We can reason that this is the worst possible case as follows: if there were
multiple sources related to $q$, then $y$ would only know that $x$ possesses one
of those sources; it would not necessarily know which one. Also, if there were
multiple peers with access to a source for $q$, then multiple peers would
respond when $y$ queries every peer about $q$. The same would occur if $x$ had
previously shared knowledge about $q$ with other peers in the network. In both
of these cases we can quantify the information gained by $y$. Let $S$ be the
event that one of the peers who has knowledge of $q$ is an origin (i.e. has
source $s$) and let $H$ be the number of peers who have the knowledge and thus
respond to the query.
If $k$ peers respond to the query, then we can express $P(S|H=k)$ in terms of
$P(S)$:
\begin{align}
	P(S|H=k)&=P(S|\text{one of the $k$ is a source})=\frac{P(S\cap\text{one of
	the $k$ is a source})}{P(\text{one of the $k$ is a source})}\nonumber\\
	&=\frac{P(S)}{1-P(\text{none of the $k$ are sources})}\label{eq:psksub}\\
	&=\frac{P(S)}{1-(1-P(S))^k}\label{eq:psk}
\end{align}
Note that we can simplify to \eqref{eq:psksub} because the event $S$ is included
in the event that one of the $k$ responders is a source. Note also that
$P(S|H=k)>P(S)$ for $P(S)\in(0,1)$. When $k=1$ (which is our previous assumption
that there is only one origin), $P(S|H=k)=1$ which agrees with our reasoning
that $y$ knows definitively that $x$ has source $s$. Thus if more than one peer
responds (either by multiple sources or by $x$ sharing knowledge), $y$ cannot be
certain whether $x$ has a source.

Going back to our original worst-case assumptions, suppose that after $y$
queries, another peer $z$ queries for information about $q$. Now we are in the
previously discussed weaker case: {\it two} peers will respond with knowledge
about $q$ ($x$ and $y$)! Thus $z$'s information about $s$ is only $P(S|H=2)<1$.
In general---where $H$ is the number of peers in the network who have knowledge
about $q$---if $H=k$ then the worst case is that the next peer who queries will
gain the information $P(S|H=k)$ about one of the responding peers possessing
$s$. The goal of a private attribution network is to provide some probability
less than $P(S|H=k)$.

% TODO make this a this theorem to reference

\subsection{Unreliable Peers}\label{sec:unreliable}

One simple approach to improving the privacy of the network is for the peers to be
unreliable i.e. let $P(\text{respond to a query about }q|\text{have source
}s)=r<1$. Now when $y$ queries the entire network, they can no longer be sure
that the peers who respond are all the peers who have the relevant knowledge.
Thus we now have a new random variable $M\ne H$ which is the number of peers who
respond to a query about $q$.

If $m$ peers respond to a query, in order to calculate the posterior, $y$ must
consider how many peers in the network actually have knowledge about $q$, which
can range from $m$ to $n-1$ (since $y$ does not know about $q$).
\begin{align}
	P(S|M=m)&=\sum_{k=m}^{n-1}P(S\cap H=k|M=m)\nonumber\\
	&=\sum_{k=m}^{n-1}\frac{P(S\cap H=k\cap M=m)}{P(M=m)}\frac{P(H=k\cap
	M=m)}{P(H=k\cap M=m)}\nonumber\\
	&=\sum_{k=m}^{n-1}P(S|H=k\cap M=m)P(H=k|M=m)\nonumber\\
	&=\sum_{k=m}^{n-1}P(S|H=k)\frac{P(M=m|H=k)P(H=k)}{P(M=m)}\label{eq:psmsimp}\\
	&=\frac{1}{P(M=m)}\sum_{k=m}^{n-1}P(S|H=k)P(M=m|H=k)P(H=k)\label{eq:psmbig}
\end{align}
where $P(S|H=k)$ is calculated as in \eqnref{psk} and we can simplify to
\eqref{eq:psmsimp} because $S$ is independent of $M$ if we know $H=k$.
We see that now the posterior probability of a source depends on the prior
distribution of knowledge in the network $H$. We also see the prior probability
of $M$, but this too can be expressed in terms of $H$:
\begin{align*}
	P(M=m)=\sum_{k=m}^{n-1}P(M=m\cap H=k)=\sum_{k=m}^{n-1}P(M=m|H=k)P(H=k)
\end{align*}

Lastly we observe that $P(M=m|H=k)$ is simply given by the binomial
distribution $b(m,k,r)$: the probability of $m$ successes out of $k$ trials where
each trial has probability $r$ of success. Substituting into \eqnref{psmbig} gives us
\begin{equation}\label{eq:psm}
	P(S|M=m)=\frac{\sum_{k=m}^{n-1}P(S|H=k)b(m,k,r)P(H=k)}{\sum_{k=m}^{n-1}b(m,k,r)P(H=k)}
\end{equation}

\begin{theorem}
	$P(S|M=m)\le P(S|H=m)$ and this inequality is strict if $r<1$.
\end{theorem}
\begin{proof}
	Suppose that $r=1$, then $P(H=m)=1$ so \eqnref{psm} simplifies to
	\begin{equation*}
		P(S|M=m)=\frac{P(S|H=m)b(m,m,r)P(H=m)}{b(m,m,r)P(H=m)}=P(S|H=m)
	\end{equation*}
	Now suppose $r<1$, by \eqnref{psk} we have that
	\begin{equation*}
		P(S|H=k+1)=\frac{P(S)}{1-(1-P(S))^{k+1}}<\frac{P(S)}{1-(1-P(S))^k}=P(S|H=k)
	\end{equation*}
	therefore
	\begin{align*}
		P(S|M=m)&=\frac{\sum_{k=m}^{n-1}P(S|H=k)b(m,k,r)P(H=k)}{\sum_{k=m}^{n-1}b(m,k,r)P(H=k)}\\
		&<\frac{\sum_{k=m}^{n-1}P(S|H=m)b(m,k,r)P(H=k)}{\sum_{k=m}^{n-1}b(m,k,r)P(H=k)}\\
		&=P(S|H=m)\frac{\sum_{k=m}^{n-1}b(m,k,r)P(H=k)}{\sum_{k=m}^{n-1}b(m,k,r)P(H=k)}\\
		&=P(S|H=m)
	\end{align*}
\end{proof}


% TODO performance impact

\subsection{Extra Responses}

Since $P(S|H=k)$ approaches $P(S)$ as $k$ increases, another method of
preserving privacy would be for the peers responding to queries to force $k$ to
be large. Specifically, whenever a peer responds to a query it also
sends that response to each other peer with probability
$\frac{t}{n-2}$. Since there are $n-2$ peers other than $x$ and $y$, we expect
that $t$ extra responses will be sent.

Applying this change in protocol to our worst-case situation, we see that $y$
still knows with $P(S|H=1)=1$ that $x$ has source $s$. However, afterwards $x$,
$y$, and $t$ random peers are expected to know about $q$ thus the next peer
to make a query has an expected advantage of only $P(S|H=t+2)$. We can minimize this
probability by making $t$ even larger: $t=n-2$ means that $x$ sends extra
responses to every other peer, and then the next peer won't query at all because
every peer already has the data!

But this view is oversimplified. Although this does minimize the posterior
probability of $x$ having $s$ following a query, it just leaks the
information earlier in the process. For example, consider a swarm of just four
peers $x$, $y$, $\beta$, and $\gamma$ and suppose that $t=2$. $y$ starts as usual by
querying every peer about $q$ ($x$, $\beta$, and $\gamma$). Only $x$ has knowledge of
$q$, so it responds to $y$ and sends extra responses with probability
$t/(n-2)=1$ to peers $\beta$ and $\gamma$. From $\beta$'s perspective, since it received a
query about $q$ from $y$, it knows that $x$ and $\gamma$ were also queried. Since a
queried peer always sends extra responses if it has the knowledge, $\beta$ knows that
every peer who sends it an extra response knows about $q$. It received an extra
response only from $x$ thus it can apply the same logic as $y$ (in
\secref{risk}) to deduce that $x$ has $s$.

Clearly, $t=n-2$ is too large. But even smaller values of $t$ leak some privacy to the
peers receiving extra responses. In fact, the situation generalizes to that of
\secref{unreliable}: when a query about $q$ is received by a peer $\beta$,
they expect to receive extra response from each peer that has knowledge of
$q$ with probability $t/(n-2)$. It is as if $\beta$ itself made the query and each
peer responds probabilistically, which is exactly the situation with unreliable
peers, only now $r=t/(n-2)$ and $M$ is the number of extra responses received by
$\beta$.

The point of sending out random extra responses would be defeated if
${P(S|M=m)>P(S|H=m)}$ because then the extra responses a peer sends out
would actually leak more private information than if it were simply queried
directly. However we showed with \eqnref{psm} that $P(S|M=m)\le P(S|H=m)$ and in
fact it is strict inequality if $r<1$ i.e. if $t<n-2$.

Thus $t$ can be any value other than $n-2$, but what should it be? We first note
a major difference from \secref{unreliable}: an attacker in this case
has some control over $r$ and thus $M$. If we have a swarm of $n$ peers,
an attacker can add $b$ malicious peers to the swarm. These malicious peers can
share information among themselves, such as who they have received extra
responses from. In effect, the $b$ peers behave like a single peer that has a
higher chance of receiving extra responses.

\begin{theorem}
	If extra responses are sent with probability $\frac{t}{n-2}$, then an
	attacker inserting malicious peers into the network can guarantee that they
	will receive extra responses with probability at most $1-1/e^t$.
\end{theorem}
\begin{proof}
Let $B$ be the event that at least one of the $b$ malicious peers is sent an extra
	response and let $n$ be the original number of nodes in the network. Then:
\begin{align*}
	P(B)&=1-P(\neg B)\\
	&=1-\left(1-P(\text{a particular malicious peer gets an extra response})\right)^b\\
	&=1-\left(1-\frac{t}{n+b-2}\right)^b\\
	\lim_{b\rightarrow\infty}P(B)&=1-\frac{1}{e^t}
\end{align*}
\end{proof}
Even at $t=5$ this probability is very close to 1. So to defend against these
attacks $t$ should be kept as small as possible.

% TODO figure out how good small t's are though

Note that this leak also relies largely on the assumption that every peer
queries every other peer in order to greedily determine as much private
information as possible. In an actual P2P network, it is not necessary for each
peer to query every other peer. Thus with non-greedy peers, when a peer (or
group of malicious peers) is
{\it not} sent an extra response it is possible that it will also {\it not} receive the
corresponding query for $q$---in which case it will be completely oblivious to the
entire exchange and no leak will occur.

\section{Evaluation}

How well does it work?

\section{Discussion \& Conclusion}

\begin{thebibliography}{9}

\bibitem{xray}
	M. L\'ecuyer, G. Ducoffe, F. Lan, A. Papancea, T. Petsios, R. Spahn, A. Chaintreau, and R. Geambasu.
	XRay: Enhancing the Webâ€™s Transparency with Differential Correlation.
	\emph{USENIX Security}, 2014.

\bibitem{privatep2p}
	M. Rogers, S. Bhatti.
	How to Disappear Completely: A Survey of Private Peer-to-Peer Networks.
	\emph{Sustaining Privacy in Autonomous Collaborative Environments (SPACE)}, 2007.

\end{thebibliography}

\end{document}
